{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing\n",
    "from tensorflow.keras import preprocessing , utils\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=open(r'DATASET.txt',encoding='utf-8').read().split(\"\\n\")\n",
    "c=docs[0].strip().split(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_for_token = list()\n",
    "answers_for_token = list()\n",
    "c=1\n",
    "for con in docs:\n",
    "    if(c==2868):\n",
    "        pass\n",
    "    else:\n",
    "        con=con.strip().split(\"\\t\")\n",
    "        questions_for_token.append(con[0])\n",
    "        answers_for_token.append(con[1])\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100 # how big is each word vector\n",
    "max_features=6000\n",
    "maxlen=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def processTweet(chat):\n",
    "    chat = chat.lower()\n",
    "    chat = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',chat)\n",
    "    chat = re.sub('@[^\\s]+','',chat)\n",
    "    chat = re.sub('[\\s]+', ' ', chat)\n",
    "    chat = re.sub(r'#([^\\s]+)', r'\\1', chat)\n",
    "    chat = re.sub(r'[\\.!:\\?\\-\\'\\\"\\\\/]', r'', chat)\n",
    "    chat = chat.strip('\\'\"')\n",
    "    return chat\n",
    "\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\", s)\n",
    "def getFeatureVector(chat):\n",
    "    chat=processTweet(chat)\n",
    "    featureVector = []\n",
    "    #split tweet into words\n",
    "    words = chat.split()\n",
    "    for w in words:\n",
    "        #replace two or more with two occurrences\n",
    "        w = replaceTwoOrMore(w)\n",
    "        #strip punctuation\n",
    "        w = w.strip('\\'\"?,.')\n",
    "        #check if the word stats with an alphabet\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "        #ignore if it is a stop word\n",
    "        if(val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return \" \".join(list(featureVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_mat(nb_words):\n",
    "    EMBEDDING_FILE=\"glove.6B.100d.txt\"\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE, encoding=\"utf8\"))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    emb_mean,emb_std\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words+1, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if (i >= max_features) or i==nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word) #here we will get embedding for each word from GloVe\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_data(questions,answers,VOCAB_SIZE,tokenizer):\n",
    "    # encoder_input_data\n",
    "    import numpy as np\n",
    "    tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "    maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "    padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen , padding='post' )\n",
    "    encoder_input_data = np.array( padded_questions )\n",
    "    #print( encoder_input_data.shape , maxlen_questions )\n",
    "\n",
    "    # decoder_input_data\n",
    "    tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "    maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "    padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen , padding='post' )\n",
    "    decoder_input_data = np.array( padded_answers )\n",
    "    #print( decoder_input_data.shape , maxlen_answers )\n",
    "\n",
    "    # decoder_output_data\n",
    "    tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "    for i in range(len(tokenized_answers)) :\n",
    "        tokenized_answers[i] = tokenized_answers[i][1:] # remove <start> take rest\n",
    "    padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen , padding='post' )\n",
    "    onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE)\n",
    "    decoder_output_data = np.array( onehot_answers )\n",
    "    #print( decoder_output_data.shape )\n",
    "    \n",
    "    return [encoder_input_data,decoder_input_data,decoder_output_data,maxlen_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_gpu():\n",
    "    #config = tf.compat.v1.ConfigProto( device_count = {'GPU': 1 , 'CPU': 56} ) \n",
    "    #sess = tf.compat.v1.Session(config=config) \n",
    "    #tf.compat.v1.keras.backend.set_session(\n",
    "    #    sess\n",
    "    #)\n",
    "    #gpu_options = tf.compat.v1.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=0.333)\n",
    "    #session = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(nb_words,embed_size,embedding_matrix):\n",
    "    encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "    encoder_embedding = tf.keras.layers.Embedding( nb_words+1, embed_size , mask_zero=True ,weights=[embedding_matrix]) (encoder_inputs)\n",
    "    encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "    encoder_states = [ state_h , state_c ]\n",
    "\n",
    "    decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "    decoder_embedding = tf.keras.layers.Embedding( nb_words+1, embed_size , mask_zero=True,weights=[embedding_matrix]) (decoder_inputs)\n",
    "    decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "    decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "\n",
    "    decoder_dense = tf.keras.layers.Dense( nb_words+1 , activation=tf.keras.activations.softmax ) \n",
    "    output = decoder_dense ( decoder_outputs )\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h_d, state_c_d = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h_d, state_c_d]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "    return model,encoder_model,encoder_inputs,encoder_states,decoder_lstm,decoder_embedding,decoder_dense,decoder_inputs,decoder_outputs,decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath = \"model_Translate_new1.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(questions,answers):\n",
    "    answers=pd.DataFrame(answers, columns=[\"Ans\"])\n",
    "    questions=pd.DataFrame(questions, columns=[\"Question\"])\n",
    "    questions[\"TokQues\"]=questions[\"Question\"].apply(getFeatureVector)\n",
    "\n",
    "    answers=np.array(answers[\"Ans\"])\n",
    "    questions=np.array(questions[\"TokQues\"])\n",
    "\n",
    "    answers_with_tags = list()\n",
    "    for i in range( len( answers ) ):\n",
    "        if type( answers[i] ) == str:\n",
    "            answers_with_tags.append( answers[i] )\n",
    "        else:\n",
    "            print(questions[i])\n",
    "            print(answers[i])\n",
    "            print(type(answers[i]))\n",
    "            questions.pop(i)\n",
    "\n",
    "    answers = list()\n",
    "    for i in range( len( answers_with_tags ) ) :\n",
    "        answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
    "    \n",
    "    \n",
    "    tokenizer = preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(questions+answers)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "\n",
    "    #embedding_matrix=emb_mat(nb_words)[0]\n",
    "    #emb_vec=emb_mat(nb_words)[1]\n",
    "\n",
    "    VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "    \n",
    "    \n",
    "    tok_out=tokenized_data(questions,answers,VOCAB_SIZE,tokenizer)\n",
    "    encoder_input_data=tok_out[0]\n",
    "    decoder_input_data=tok_out[1]\n",
    "    decoder_output_data=tok_out[2]\n",
    "    maxlen_answers=tok_out[3]\n",
    "    \n",
    "    return [encoder_input_data,decoder_input_data,decoder_output_data,maxlen_answers,nb_words,word_index,tokenizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparation done\n",
      "Encoder done\n",
      "Decoder done\n",
      "(2867, 100) (2867, 100) (2867, 100, 5375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3378: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 23:37:46.430830: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287/287 [==============================] - ETA: 0s - loss: 5.2867 - accuracy: 0.2406\n",
      "Epoch 1: loss improved from inf to 5.28666, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 64s 206ms/step - loss: 5.2867 - accuracy: 0.2406\n",
      "Epoch 2/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 4.6555 - accuracy: 0.2843\n",
      "Epoch 2: loss improved from 5.28666 to 4.65551, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 61s 213ms/step - loss: 4.6555 - accuracy: 0.2843\n",
      "Epoch 3/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 4.3699 - accuracy: 0.3157\n",
      "Epoch 3: loss improved from 4.65551 to 4.36989, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 64s 222ms/step - loss: 4.3699 - accuracy: 0.3157\n",
      "Epoch 4/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 4.1338 - accuracy: 0.3445\n",
      "Epoch 4: loss improved from 4.36989 to 4.13383, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 64s 222ms/step - loss: 4.1338 - accuracy: 0.3445\n",
      "Epoch 5/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 3.9274 - accuracy: 0.3696\n",
      "Epoch 5: loss improved from 4.13383 to 3.92740, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 62s 216ms/step - loss: 3.9274 - accuracy: 0.3696\n",
      "Epoch 6/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 3.7315 - accuracy: 0.3940\n",
      "Epoch 6: loss improved from 3.92740 to 3.73155, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 61s 214ms/step - loss: 3.7315 - accuracy: 0.3940\n",
      "Epoch 7/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 3.5479 - accuracy: 0.4182\n",
      "Epoch 7: loss improved from 3.73155 to 3.54786, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 61s 214ms/step - loss: 3.5479 - accuracy: 0.4182\n",
      "Epoch 8/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 3.3740 - accuracy: 0.4419\n",
      "Epoch 8: loss improved from 3.54786 to 3.37399, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 63s 221ms/step - loss: 3.3740 - accuracy: 0.4419\n",
      "Epoch 9/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 3.2116 - accuracy: 0.4660\n",
      "Epoch 9: loss improved from 3.37399 to 3.21160, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 62s 217ms/step - loss: 3.2116 - accuracy: 0.4660\n",
      "Epoch 10/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 3.0573 - accuracy: 0.4889\n",
      "Epoch 10: loss improved from 3.21160 to 3.05732, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 60s 210ms/step - loss: 3.0573 - accuracy: 0.4889\n",
      "Epoch 11/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 2.9167 - accuracy: 0.5164\n",
      "Epoch 11: loss improved from 3.05732 to 2.91672, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 63s 219ms/step - loss: 2.9167 - accuracy: 0.5164\n",
      "Epoch 12/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 2.7792 - accuracy: 0.5412\n",
      "Epoch 12: loss improved from 2.91672 to 2.77915, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 62s 215ms/step - loss: 2.7792 - accuracy: 0.5412\n",
      "Epoch 13/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 2.6566 - accuracy: 0.5679\n",
      "Epoch 13: loss improved from 2.77915 to 2.65661, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 63s 219ms/step - loss: 2.6566 - accuracy: 0.5679\n",
      "Epoch 14/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 2.5333 - accuracy: 0.5920\n",
      "Epoch 14: loss improved from 2.65661 to 2.53331, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 62s 216ms/step - loss: 2.5333 - accuracy: 0.5920\n",
      "Epoch 15/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 2.4220 - accuracy: 0.6131\n",
      "Epoch 15: loss improved from 2.53331 to 2.42200, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 61s 214ms/step - loss: 2.4220 - accuracy: 0.6131\n",
      "Epoch 16/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 2.3187 - accuracy: 0.6321\n",
      "Epoch 16: loss improved from 2.42200 to 2.31874, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 63s 218ms/step - loss: 2.3187 - accuracy: 0.6321\n",
      "Epoch 17/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 2.2246 - accuracy: 0.6541\n",
      "Epoch 17: loss improved from 2.31874 to 2.22465, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 64s 222ms/step - loss: 2.2246 - accuracy: 0.6541\n",
      "Epoch 18/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 2.1331 - accuracy: 0.6738\n",
      "Epoch 18: loss improved from 2.22465 to 2.13306, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 64s 221ms/step - loss: 2.1331 - accuracy: 0.6738\n",
      "Epoch 19/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 2.0394 - accuracy: 0.6905\n",
      "Epoch 19: loss improved from 2.13306 to 2.03940, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 64s 224ms/step - loss: 2.0394 - accuracy: 0.6905\n",
      "Epoch 20/20\n",
      "287/287 [==============================] - ETA: 0s - loss: 1.9550 - accuracy: 0.7047\n",
      "Epoch 20: loss improved from 2.03940 to 1.95503, saving model to model_Translate_new1.h5\n",
      "287/287 [==============================] - 63s 221ms/step - loss: 1.9550 - accuracy: 0.7047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x287df0730>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prepared_data=prepare_data(questions_for_token,answers_for_token)\n",
    "print(\"Preparation done\")\n",
    "encoder_input_data=Prepared_data[0]\n",
    "print(\"Encoder done\")\n",
    "decoder_input_data=Prepared_data[1]\n",
    "decoder_output_data=Prepared_data[2]\n",
    "print(\"Decoder done\")\n",
    "maxlen_answers=Prepared_data[3]\n",
    "nb_words=Prepared_data[4]\n",
    "word_index=Prepared_data[5]\n",
    "tokenizer=Prepared_data[6]\n",
    "print(encoder_input_data.shape, decoder_input_data.shape, decoder_output_data.shape)\n",
    "\n",
    "embedding_matrix=emb_mat(nb_words)\n",
    "#model=get_model(nb_words,embed_size,embedding_matrix)[0]\n",
    "#encoder_model=get_model(nb_words,embed_size,embedding_matrix)[1]\n",
    "#encoder_inputs=get_model(nb_words,embed_size,embedding_matrix)[2]\n",
    "#encoder_states=get_model(nb_words,embed_size,embedding_matrix)[3]\n",
    "#decoder_lstm=get_model(nb_words,embed_size,embedding_matrix)[4]\n",
    "#decoder_embedding=get_model(nb_words,embed_size,embedding_matrix)[5]\n",
    "#decoder_dense=get_model(nb_words,embed_size,embedding_matrix)[6]\n",
    "#decoder_inputs=get_model(nb_words,embed_size,embedding_matrix)[7]\n",
    "#decoder_outputs=get_model(nb_words,embed_size,embedding_matrix)[8]\n",
    "#decoder_model=get_model(nb_words,embed_size,embedding_matrix)[9]\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( nb_words+1, embed_size , mask_zero=True, weights=[embedding_matrix]) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( nb_words+1, embed_size , mask_zero=True,weights=[embedding_matrix]) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "\n",
    "decoder_dense = tf.keras.layers.Dense( nb_words+1 , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=10, epochs=20, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.models import load_model\n",
    "#Loaded_model = load_model(r'model_Translate_new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model , dec_model = make_inference_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter question : hey how are you\n",
      "Sorry, I don't understand. Please try again.\n",
      "Enter question : hey how are you\n",
      "Sorry, I don't understand. Please try again.\n",
      "Enter question : how are you\n",
      "1/1 [==============================] - 1s 874ms/step\n",
      "1/1 [==============================] - 1s 978ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "आप कैसे हो\n",
      "Enter question : where have you been\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "तुम कहाँ थे\n",
      "Enter question : what is the time\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "अभी क्या है\n",
      "Enter question : what's the time\n",
      "Sorry, I don't understand. Please try again.\n",
      "Enter question : what is the time \n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "अभी क्या है\n",
      "Enter question : what is your problem\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "तुम्हारी क्या है\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    try:\n",
    "        states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "        empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "        stop_condition = False\n",
    "        decoded_translation = ''\n",
    "        while not stop_condition :\n",
    "            dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "            sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "            sampled_word = None\n",
    "            for word , index in tokenizer.word_index.items() :\n",
    "                if sampled_word_index == index :\n",
    "                    decoded_translation += ' {}'.format( word )\n",
    "                    sampled_word = word\n",
    "            \n",
    "            if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "                stop_condition = True\n",
    "                \n",
    "            empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "            empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "            states_values = [ h , c ] \n",
    "\n",
    "        print( \" \".join(decoded_translation.strip().split(\" \")[:-1]) )\n",
    "    except:\n",
    "        print(\"Sorry, I don't understand. Please try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still a better accuracy with 20 epoches. We can make it better with more data and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
